{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深入 Deep Learning\n",
    "\n",
    "## Convolution Neural Networks\n",
    "\n",
    "卷积神经网络 (CNNs)在计算机视觉任务中获得了巨大的成功。一个卷积神经网络神经网络由以下部分构成：\n",
    "\n",
    "* Convolution layers\n",
    "* Pooling layers\n",
    "* Dense layers (for final prediction)\n",
    "\n",
    "在本次实验中, 我们将运用 CNNs 来学习 Cifar10 数据. 像数据集 MNIST一样, Cifar10 是计算机视觉中的另一个数据集. 通过该数据集，我们将学习：\n",
    "\n",
    "* 如何做简单的数据探索\n",
    "* 如何定义一个 CNN 模型\n",
    "* 如何形式化数据满足训练模型需要\n",
    "* 如何训练一个模型\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一步导入相关的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    "\n",
    "# Fixing the random seed\n",
    "fix_random_seed(4321)\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二步 加载和探索数据\n",
    "\n",
    "首先使用`tensorflow-datasets` 库加载数据集，然后输出数据集中的每个第一项，查看数据内容，将看到图像数据在[0,255]之间变化。最后，我们将绘制一些图像，以理解我们必须使用的类和图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4.2\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "# Loading the CIFAR10 dataset\n",
    "data = tfds.load('cifar10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4.2\n",
    "\n",
    "# Let's see the first element in the training set\n",
    "for i in data[\"train\"].take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三步 数据展示\n",
    "\n",
    "绘制一些数据以及这些图像类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4.2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Take 10 samples randomly to plot\n",
    "sample_images, sample_labels = [],[]\n",
    "for d in data[\"train\"].shuffle(100, seed=4321).take(10):\n",
    "    sample_images.append(d[\"image\"].numpy())\n",
    "    sample_labels.append(d[\"label\"].numpy())\n",
    "\n",
    "# Creating a label map mapping the integer label to the string\n",
    "label_map = dict(zip(\n",
    "    list(range(10)),\n",
    "    [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]))\n",
    "\n",
    "# Plotting the images\n",
    "f, axes = plt.subplots(2, 5, figsize=(9,4))\n",
    "for i, (img, lbl) in enumerate(zip(sample_images, sample_labels)):\n",
    "    r, c = i//5, i%5\n",
    "    axes[r,c].imshow(img,cmap='gray')\n",
    "    axes[r,c].axis('off')\n",
    "    axes[r,c].set_title(\"Label: {}\".format(label_map[lbl]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四步 定义模型\n",
    "\n",
    "定义一个CNN模型：我们试图运行这个模型时会出现一个错误。这是因为，我们需要注意模型每一层的输出大小。在这种情况下，我们创建的模型导致了无效的高度和宽度维度 **causing an error**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4.2\n",
    "# Code listing 4.2\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Defining the CNN model\n",
    "cnn = models.Sequential(\n",
    "    [layers.Conv2D(filters=16,kernel_size=(9,9), strides=(2,2), activation='relu', input_shape=(32,32,3)), # 32->12\n",
    "     layers.Conv2D(32, (7,7), activation='relu'), # 12 -> 6\n",
    "     layers.Conv2D(64, (7,7), activation='relu'), # 6 -> -1\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五步 以正确的方式定义模型\n",
    "\n",
    "解决这个错误，修正上面的模型，这样我们就不会有任何错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4.2\n",
    "# Code listing 4.3\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Defining the CNN model without any errores\n",
    "cnn = models.Sequential(\n",
    "    [layers.Conv2D(filters=16,kernel_size=(3,3), strides=(2,2), activation='relu', padding='same', input_shape=(32,32,3)), # 32->16\n",
    "     layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding='same'), # 16->8\n",
    "     layers.Conv2D(32, (3,3), activation='relu', padding='same'), # 8 -> 8\n",
    "     layers.MaxPool2D(pool_size=(2,2), strides=(2,2), padding='same'), # 8->4\n",
    "     layers.Flatten(),\n",
    "     layers.Dense(64, activation='relu'),\n",
    "     layers.Dense(32, activation='relu'),\n",
    "     layers.Dense(10, activation='softmax')]\n",
    ")\n",
    "\n",
    "# Compiling the model\n",
    "cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第六步 训练模型\n",
    "最后，可以训练模型，需要稍微改变数据集对象，把图像转换为' float32 '(原来是' uint8 ')，并将标签转换为one-hot编码向量。为此，将使用' tf.data.DataSet.map() '函数。\n",
    "\n",
    "然后训练模型，模型的训练精度在稳步增长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4.2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def format_data(x, depth):\n",
    "    \"\"\" Create a tuple where 1st element is a batch of images \n",
    "    and the second is a batch of onehot encoded vectors\"\"\"\n",
    "    return (tf.cast(x[\"image\"], 'float32'), tf.one_hot(x[\"label\"], depth=depth))\n",
    "\n",
    "# Map the dataset using the function\n",
    "tr_data = data[\"train\"].map(lambda x: format_data(x, depth=10)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in tr_data.take(1):\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4.2\n",
    "# Fit the data\n",
    "history = cnn.fit(tr_data,epochs=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
